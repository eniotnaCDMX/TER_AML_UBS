{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e5620fd-3f2f-48be-9e18-553a2571be57",
   "metadata": {},
   "source": [
    "# Notebook 4 - Graph Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4c687-074a-4cb9-81f8-85390f074adf",
   "metadata": {},
   "source": [
    "### On importe les libraries nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cb2234-1f42-4eef-8db8-d840a832f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce03fd4-d71a-4b69-9c32-29d915f75193",
   "metadata": {},
   "source": [
    "## On importe les données nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc64c8d-488c-41b1-8cdd-deb14f0248c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On importe seulement les colonnes utiles\n",
    "accounts = pd.read_csv(\"accounts.csv\")\n",
    "transac = pd.read_csv(\"transactions.csv\")\n",
    "alerts = pd.read_csv(\"alerts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df029d6-7be2-4613-b62e-a8aaf8b14af6",
   "metadata": {},
   "source": [
    "## On prépare nous données pour qu'elles puissent être modélisée sous forme de graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e5eee-c19a-4538-8d6f-d666292cd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare edge_index\n",
    "sender_ids = torch.tensor(transac['SENDER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "receiver_ids = torch.tensor(transac['RECEIVER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "edge_index = torch.stack([sender_ids, receiver_ids], dim=0)\n",
    "\n",
    "# Prepare node features (INIT_BALANCE)\n",
    "node_features = torch.tensor(accounts[['ACCOUNT_ID']].values, dtype=torch.float)\n",
    "\n",
    "# Prepare edge features (TX_AMOUNT and TIMESTAMP)\n",
    "edge_attr = torch.tensor(transac[['TX_AMOUNT', 'TIMESTAMP']].values, dtype=torch.float)\n",
    "\n",
    "# Prepare labels (IS_FRAUD)\n",
    "labels = torch.tensor(accounts['IS_FRAUD'].astype(int).values, dtype=torch.long)\n",
    "\n",
    "# Create the PyTorch Geometric data object\n",
    "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382aa70e-6864-4830-8836-6b8dc12d821b",
   "metadata": {},
   "source": [
    "## On crée un ensemble d'entrainement, de validation et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd2e8f-9d65-4cd7-9c31-912418154f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def random_split_transd_adapted(node_features, labels, edge_index, edge_attr, train_size, test_size, device='cpu', seed=42):\n",
    "    \"\"\"\n",
    "    Split the graph into training, validation, and test sets based on node indices.\n",
    "    \n",
    "    Args:\n",
    "        node_features (torch.Tensor): Tensor of node features.\n",
    "        labels (torch.Tensor): Tensor of node labels.\n",
    "        edge_index (torch.Tensor): Tensor representing the graph's edges (connectivity).\n",
    "        edge_attr (torch.Tensor): Tensor of edge features.\n",
    "        train_size (float): The proportion of nodes to include in the training set.\n",
    "        test_size (float): The proportion of nodes to include in the test set.\n",
    "        device (str): Device to store the data on ('cpu' or 'cuda').\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        data (Data): A PyTorch Geometric Data object containing node features, edge_index, edge_attr, and labels.\n",
    "        splits (dict): A dictionary with indices for the training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # Ensure reproducibility\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Total number of nodes\n",
    "    num_nodes = labels.size(0)\n",
    "\n",
    "    # Generate node indices\n",
    "    node_indices = np.arange(num_nodes)\n",
    "\n",
    "    # Split indices into training, validation, and test sets\n",
    "    train_index, temp_index = train_test_split(node_indices, train_size=train_size, random_state=seed, stratify=labels.numpy())\n",
    "    val_index, test_index = train_test_split(temp_index, test_size=test_size / (1 - train_size), random_state=seed, stratify=labels[temp_index].numpy())\n",
    "\n",
    "    # Construct the PyTorch Geometric Data object\n",
    "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=labels).to(device)\n",
    "    \n",
    "    # Count the number of IS_FRAUD = 1 in each set\n",
    "    num_fraud_train = torch.sum(labels[train_index] == 1).item()\n",
    "    num_fraud_val = torch.sum(labels[val_index] == 1).item()\n",
    "    num_fraud_test = torch.sum(labels[test_index] == 1).item()\n",
    "    \n",
    "    # Print the counts\n",
    "    print(f\"Training set size: {len(train_index)}, IS_FRAUD=1: {num_fraud_train}\")\n",
    "    print(f\"Validation set size: {len(val_index)}, IS_FRAUD=1: {num_fraud_val}\")\n",
    "    print(f\"Test set size: {len(test_index)}, IS_FRAUD=1: {num_fraud_test}\")\n",
    "    \n",
    "    # Return the data object and the indices for each split\n",
    "    return data, {'train': train_index, 'val': val_index, 'test': test_index}\n",
    "\n",
    "# Example of how to use the function with your data\n",
    "data, splits = random_split_transd_adapted(\n",
    "    node_features=node_features, \n",
    "    labels=labels, \n",
    "    edge_index=edge_index, \n",
    "    edge_attr=edge_attr, \n",
    "    train_size=0.8, \n",
    "    test_size=0.1, \n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu', \n",
    "    seed=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b9144-892c-451d-9d30-d34f043e56f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7197a-4cae-45b5-b46f-c5bf8b649148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6366a31-8370-4906-89f1-129abbb7aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dichotomy (binary search) function to find the optimal threshold\n",
    "def find_optimal_threshold(y_prob, y_true, limit_recall, tol=1e-4):\n",
    "    low, high = 0.0, 1.0\n",
    "    best_threshold = 0.5\n",
    "    while high - low > tol:\n",
    "        mid = (low + high) / 2.0\n",
    "        y_pred = (y_prob >= mid).astype(int)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        \n",
    "        if recall < limit_recall:\n",
    "            high = mid\n",
    "        else:\n",
    "            best_threshold = mid\n",
    "            low = mid\n",
    "            \n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "# Function to calculate SAR Conversion Rate\n",
    "def sar_conversion_rate(y_true, y_pred):\n",
    "    positive_predictions = np.sum(y_pred)\n",
    "    if positive_predictions == 0:\n",
    "        return 0\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    return true_positives / positive_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42f001-bcb3-4a81-bf7f-c14d64c77301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4bc3d-e69e-4ace-96b1-4cec21d949be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f71833-82e5-4908-bbe1-b352d7ac8d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301e954-1ba3-4f63-9ee6-83bc4b2f295c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193c99f-6480-464e-bf5c-d028b235b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "from sklearn.metrics import recall_score, make_scorer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming NUM_EDGE_FEATURES = 2\n",
    "NUM_EDGE_FEATURES = 2\n",
    "\n",
    "# Define the GAT model with edge features\n",
    "class GDPModel(nn.Module):\n",
    "    def __init__(self, num_features=3, hidden_size=32, target_size=1):\n",
    "        super(GDPModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.target_size = target_size\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATConv(self.num_features, self.hidden_size, edge_dim=NUM_EDGE_FEATURES),\n",
    "            GATConv(self.hidden_size, self.hidden_size, edge_dim=NUM_EDGE_FEATURES)\n",
    "        ])\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr=edge_attr)\n",
    "        x = self.linear(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out.view(-1), data.y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(model, data, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        preds = (out.view(-1) > threshold).long()\n",
    "        correct = preds.eq(data.y).sum().item()\n",
    "        return correct / data.num_nodes\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_dist = {\n",
    "    'hidden_size': randint(16, 128),\n",
    "    'lr': uniform(0.001, 0.01),\n",
    "    'dropout': uniform(0.1, 0.5),\n",
    "    'epochs': randint(1, 50)\n",
    "}\n",
    "\n",
    "# Randomized search for hyperparameter tuning\n",
    "def randomized_search(data, param_dist, n_iter=10, seed=42):\n",
    "    results = []\n",
    "    sampler = ParameterSampler(param_dist, n_iter=n_iter, random_state=seed)\n",
    "    \n",
    "    for params in sampler:\n",
    "        print(f\"Training with params: {params}\")\n",
    "        model = GDPModel(num_features=data.x.size(1), hidden_size=params['hidden_size']).to(data.x.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(params['epochs']):\n",
    "            loss = train(model, data, optimizer, criterion)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        acc = evaluate(model, data)\n",
    "        results.append((acc, params))\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\\n\")\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
    "\n",
    "# Load data and splits (assuming you have already run the split code)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data, splits = random_split_transd_adapted(node_features, labels, edge_index, edge_attr, train_size=0.8, test_size=0.1, device=device)\n",
    "\n",
    "# Perform randomized search\n",
    "best_acc, best_params = randomized_search(data, param_dist)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = GDPModel(num_features=data.x.size(1), hidden_size=best_params['hidden_size']).to(device)\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the best model\n",
    "for epoch in range(best_params['epochs']):\n",
    "    train(best_model, data, optimizer, criterion)\n",
    "\n",
    "# Final evaluation on the test set\n",
    "test_acc = evaluate(best_model, data)\n",
    "print(f\"Test Set Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8da4d6-e61c-4f03-9a8b-bb0779009f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ebec9d-df08-4c58-8c35-b57fd6744dcd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b88c8-14d1-4214-9aa1-94df77cec093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc82a092-c512-499a-ad45-f9c94637db33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c4278-0699-4d13-8f5b-9fee0e8f063e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed1fd8-bdee-4103-acc8-4346f75dd976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd6dc9-4531-49cd-b3ec-758b0ac204a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536cf3f-bbc2-4010-887d-289526febc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af192f38-1c9e-4739-be82-9d551b5cd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Assuming `data` is your Data object with x, edge_index, edge_attr, and y\n",
    "model = GCN(num_node_features=data.num_node_features, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Test Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a65fc-5f64-4b94-9c18-f3907c464788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Calculate conversion rate\n",
    "def sar_conversion_rate(y_true, y_pred):\n",
    "    positive_predictions = (y_pred == 1).sum().item()\n",
    "    if positive_predictions == 0:\n",
    "        return 0\n",
    "    true_positives = ((y_pred == 1) & (y_true == 1)).sum().item()\n",
    "    return true_positives / positive_predictions\n",
    "\n",
    "\n",
    "# Step 1: Create Masks\n",
    "num_nodes = data.num_nodes\n",
    "num_train = int(num_nodes * 0.8)\n",
    "num_val = int(num_nodes * 0.1)\n",
    "num_test = num_nodes - num_train - num_val\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "indices = torch.randperm(num_nodes)\n",
    "train_mask[indices[:num_train]] = True\n",
    "val_mask[indices[num_train:num_train + num_val]] = True\n",
    "test_mask[indices[num_train + num_val:]] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Step 2: Define GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Step 3: Train and Evaluate the Model\n",
    "model = GCN(num_node_features=data.num_node_features, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    probabilities = torch.softmax(out, dim=1)[:, 1]\n",
    "    pred = (probabilities > 0.5).long()\n",
    "    recall = recall_score(data.y[data.val_mask].cpu(), pred[data.val_mask].cpu(), pos_label=1)\n",
    "    conversion_rate = sar_conversion_rate(data.y[data.val_mask], pred[data.val_mask])\n",
    "    print(f'Epoch {epoch}: Train Loss: {loss.item()}, Recall (Class 1): {recall:.4f}, Conversion Rate: {conversion_rate:.4f}')\n",
    "\n",
    "\n",
    "# Testing after all epochs are done\n",
    "model.eval()\n",
    "out = model(data)\n",
    "probabilities = torch.softmax(out, dim=1)[:, 1]  # Probabilities for class 1\n",
    "threshold = 0.5  # Set your decision threshold\n",
    "\n",
    "# Make predictions based on threshold\n",
    "pred = (probabilities > threshold).long()\n",
    "\n",
    "# Calculate recall for class 1\n",
    "recall = recall_score(data.y[data.test_mask].cpu(), pred[data.test_mask].cpu(), pos_label=1)\n",
    "\n",
    "conversion_rate = sar_conversion_rate(data.y[data.test_mask], pred[data.test_mask])\n",
    "\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "print(f'Recall (Class 1): {recall:.4f}')\n",
    "print(f'Conversion Rate: {conversion_rate:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680477d-e63f-4c8f-9e78-d8cbe90a4d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa116e-4db0-4e04-af43-c218272299ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a6db2-b176-455c-a63e-6e59100c382f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ff145-9826-4d68-a628-191bd0d95d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd62db7-cf89-4ddc-bad0-1ffc5ed8984c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44785f-c882-4e6c-a28b-c84c55402df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed556d-1ce3-4336-9a4b-34ba773bebda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e388319-e87c-4bc7-a989-dfc312e33464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0711c-1f0e-4f6c-b49b-b57ac0fa9c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708659d9-ed1d-4c1d-b678-1f4425373dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dbcbd-c535-4608-9558-a550210168c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the GAT model with edge features\n",
    "class GDPModel(nn.Module):\n",
    "    def __init__(self, num_features=3, hidden_size=32, target_size=1):\n",
    "        super(GDPModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.target_size = target_size\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATConv(self.num_features, self.hidden_size, edge_dim=NUM_EDGE_FEATURES),\n",
    "            GATConv(self.hidden_size, self.hidden_size, edge_dim=NUM_EDGE_FEATURES)\n",
    "        ])\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr=edge_attr)\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)  # Use sigmoid since it's binary classification\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out.view(-1), data.y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model (focus on recall only)\n",
    "def evaluate(model, data, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        preds = (out.view(-1) > threshold).long()\n",
    "        recall = recall_score(data.y.cpu(), preds.cpu())\n",
    "        return recall\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_dist = {\n",
    "    'hidden_size': randint(16, 128),\n",
    "    'lr': uniform(0.0001, 0.01),\n",
    "    'dropout': uniform(0.1, 0.5),\n",
    "    'epochs': randint(10, 50)\n",
    "}\n",
    "\n",
    "# Randomized search for hyperparameter tuning\n",
    "def randomized_search(data, param_dist, n_iter=10, seed=42):\n",
    "    results = []\n",
    "    sampler = ParameterSampler(param_dist, n_iter=n_iter, random_state=seed)\n",
    "    \n",
    "    for params in sampler:\n",
    "        print(f\"Training with params: {params}\")\n",
    "        model = GDPModel(num_features=data.x.size(1), hidden_size=params['hidden_size']).to(data.x.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(params['epochs']):\n",
    "            train(model, data, optimizer, criterion)\n",
    "        \n",
    "        recall = evaluate(model, data)\n",
    "        results.append((recall, params))\n",
    "        print(f\"Validation Recall: {recall:.4f}\\n\")\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
    "\n",
    "# Load data and splits (assuming you have already run the split code)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data, splits = random_split_transd_adapted(node_features, labels, edge_index, edge_attr, train_size=0.8, test_size=0.1, device=device)\n",
    "\n",
    "# Perform randomized search for hyperparameter tuning\n",
    "best_recall, best_params = randomized_search(data, param_dist)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation Recall: {best_recall:.4f}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = GDPModel(num_features=data.x.size(1), hidden_size=best_params['hidden_size']).to(device)\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(best_params['epochs']):\n",
    "    train(best_model, data, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c849d9a-9b72-438b-bf83-90fa3272e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate conversion rate\n",
    "def conversion_rate(y_true, y_pred):\n",
    "    positive_predictions = np.sum(y_pred)\n",
    "    if positive_predictions == 0:\n",
    "        return 0\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    return true_positives / positive_predictions\n",
    "\n",
    "# Dichotomy (binary search) function to find the optimal threshold\n",
    "def find_optimal_threshold(y_prob, y_true, limit_recall, tol=1e-4):\n",
    "    low, high = 0.0, 1.0\n",
    "    best_threshold = 0.5\n",
    "    while high - low > tol:\n",
    "        mid = (low + high) / 2.0\n",
    "        y_pred = (y_prob >= mid).astype(int)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        \n",
    "        if recall < limit_recall:\n",
    "            high = mid\n",
    "        else:\n",
    "            best_threshold = mid\n",
    "            low = mid\n",
    "            \n",
    "    return best_threshold\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "y_prob_test = best_model(data).view(-1).cpu().detach().numpy()\n",
    "\n",
    "# Calculate SAR Conversion Rate and Recall for each threshold\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "sar_rates = []\n",
    "recalls = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_test = (y_prob_test >= threshold).astype(int)\n",
    "    sar_rate = conversion_rate(labels.cpu().numpy(), y_pred_test)\n",
    "    recall = recall_score(labels.cpu().numpy(), y_pred_test)\n",
    "    sar_rates.append(sar_rate)\n",
    "    recalls.append(recall)\n",
    "\n",
    "# Define the recall limit\n",
    "limit_recall = 0.85  # Adjust as needed\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel('Threshold')\n",
    "ax1.set_ylabel('SAR Conversion Rate', color='tab:blue')\n",
    "ax1.plot(thresholds, sar_rates, color='tab:blue', label='SAR Conversion Rate')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Recall (Class 1)', color='tab:orange')\n",
    "ax2.plot(thresholds, recalls, color='tab:orange', label='Recall (Class 1)')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "ax2.axhline(y=limit_recall, color='red', linestyle='--', linewidth=1, label='Recall Limit')\n",
    "fig.tight_layout()\n",
    "plt.title('Threshold vs SAR Conversion Rate & Recall (Class 1)')\n",
    "plt.show()\n",
    "\n",
    "# Find and print the optimal threshold and corresponding SAR Conversion Rate\n",
    "optimal_threshold = find_optimal_threshold(y_prob_test, labels.cpu().numpy(), limit_recall)\n",
    "y_pred_optimal = (y_prob_test >= optimal_threshold).astype(int)\n",
    "optimal_sar = conversion_rate(labels.cpu().numpy(), y_pred_optimal)\n",
    "\n",
    "print(f\"Optimal Threshold for Recall Limit {limit_recall}: {optimal_threshold:.4f}\")\n",
    "print(f\"SAR Conversion Rate at Optimal Threshold: {optimal_sar:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1ed82-a634-4b8a-91a9-3fa65e04ab21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01195e5-c66c-4ae9-abc4-fe802ad07540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45650faf-9348-44c1-a887-7379cbde6578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff6201-d58d-4222-a3cb-f8261e71f215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622bd7de-219c-4de3-a531-76415a60b64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7955a-70e3-497a-83d9-e0f194638084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.model_selection import ParameterSampler, StratifiedKFold\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the GAT model with edge features\n",
    "class GDPModel(nn.Module):\n",
    "    def __init__(self, num_features=3, hidden_size=32, target_size=1):\n",
    "        super(GDPModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.target_size = target_size\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATConv(self.num_features, self.hidden_size, edge_dim=NUM_EDGE_FEATURES),\n",
    "            GATConv(self.hidden_size, self.hidden_size, edge_dim=NUM_EDGE_FEATURES)\n",
    "        ])\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr)  # Adding edge features here\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr=edge_attr)  # Adding edge features here\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)  # Use sigmoid for binary classification\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out.view(-1), data.y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model (focus on recall only)\n",
    "def evaluate(model, data, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        preds = (out.view(-1) > threshold).long()\n",
    "        recall = recall_score(data.y.cpu(), preds.cpu())\n",
    "        return recall\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_dist = {\n",
    "    'hidden_size': randint(16, 128),\n",
    "    'lr': uniform(0.0001, 0.01),\n",
    "    'dropout': uniform(0.1, 0.5),\n",
    "    'epochs': randint(10, 150)\n",
    "}\n",
    "\n",
    "# Randomized search for hyperparameter tuning\n",
    "def randomized_search(data, param_dist, n_iter=10, seed=42):\n",
    "    results = []\n",
    "    sampler = ParameterSampler(param_dist, n_iter=n_iter, random_state=seed)\n",
    "    \n",
    "    for params in sampler:\n",
    "        print(f\"Training with params: {params}\")\n",
    "        model = GDPModel(num_features=data.x.size(1), hidden_size=params['hidden_size']).to(data.x.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(params['epochs']):\n",
    "            train(model, data, optimizer, criterion)\n",
    "        \n",
    "        recall = evaluate(model, data)\n",
    "        results.append((recall, params))\n",
    "        print(f\"Validation Recall: {recall:.4f}\\n\")\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
    "\n",
    "# Load data and splits (assuming you have already run the split code)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data, splits = random_split_transd_adapted(node_features, labels, edge_index, edge_attr, train_size=0.8, test_size=0.1, device=device)\n",
    "\n",
    "# Perform randomized search for hyperparameter tuning\n",
    "best_recall, best_params = randomized_search(data, param_dist)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation Recall: {best_recall:.4f}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = GDPModel(num_features=data.x.size(1), hidden_size=best_params['hidden_size']).to(device)\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(best_params['epochs']):\n",
    "    train(best_model, data, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd11ce3-14d2-43b0-84fc-d187b7feb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate SAR Conversion Rate\n",
    "def sar_conversion_rate(y_true, y_pred):\n",
    "    positive_predictions = np.sum(y_pred)\n",
    "    if positive_predictions == 0:\n",
    "        return 0\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    return true_positives / positive_predictions\n",
    "\n",
    "# Dichotomy (binary search) function to find the optimal threshold\n",
    "def find_optimal_threshold(y_prob, y_true, limit_recall, tol=1e-4):\n",
    "    low, high = 0.0, 1.0\n",
    "    best_threshold = 0.5\n",
    "    while high - low > tol:\n",
    "        mid = (low + high) / 2.0\n",
    "        y_pred = (y_prob >= mid).astype(int)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        \n",
    "        if recall < limit_recall:\n",
    "            high = mid\n",
    "        else:\n",
    "            best_threshold = mid\n",
    "            low = mid\n",
    "            \n",
    "    return best_threshold\n",
    "\n",
    "# Cross-validation setup with stratification\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "optimal_thresholds = []\n",
    "\n",
    "# Cross-validation process\n",
    "for fold_num, (train_index, val_index) in enumerate(skf.split(data.x.cpu().numpy(), labels.cpu().numpy()), start=1):\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_index] = True\n",
    "    val_mask[val_index] = True\n",
    "    \n",
    "    # Train model on the current fold\n",
    "    best_model.train()\n",
    "    for epoch in range(best_params['epochs']):\n",
    "        train(best_model, data, optimizer, criterion)\n",
    "    \n",
    "    # Predict probabilities on the validation fold\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_prob_val_fold = best_model(data).view(-1)[val_mask].cpu().detach().numpy()\n",
    "        y_true_val_fold = labels[val_mask].cpu().detach().numpy()\n",
    "\n",
    "    # Calculate SAR Conversion Rate and Recall for each threshold\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    sar_rates = []\n",
    "    recalls = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_val = (y_prob_val_fold >= threshold).astype(int)\n",
    "        sar_rate = sar_conversion_rate(y_true_val_fold, y_pred_val)\n",
    "        recall = recall_score(y_true_val_fold, y_pred_val)\n",
    "        sar_rates.append(sar_rate)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Objective recall for the current fold\n",
    "    objective_recall = np.random.uniform(min_recall_tuning, 1.0)\n",
    "    print(f\"Fold {fold_num}: Objective Recall Target = {objective_recall:.4f}\")\n",
    "    \n",
    "    # Find the optimal threshold using the dichotomy method for this fold\n",
    "    optimal_threshold = find_optimal_threshold(y_prob_val_fold, y_true_val_fold, objective_recall)\n",
    "    optimal_thresholds.append(optimal_threshold)\n",
    "\n",
    "    # Plot SAR Conversion Rate and Recall for each threshold\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('SAR Conversion Rate', color='tab:blue')\n",
    "    ax1.plot(thresholds, sar_rates, color='tab:blue', label='SAR Conversion Rate')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('Recall (Class 1)', color='tab:orange')\n",
    "    ax2.plot(thresholds, recalls, color='tab:orange', label='Recall (Class 1)')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Highlight the selected threshold and the objective recall\n",
    "    ax1.axvline(x=optimal_threshold, color='red', linestyle='--', linewidth=1, label='Selected Threshold')\n",
    "    ax2.axhline(y=objective_recall, color='green', linestyle='--', linewidth=1, label='Objective Recall')\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(f'Threshold vs SAR Conversion Rate & Recall (Class 1) - Fold {fold_num}')\n",
    "    plt.show()\n",
    "\n",
    "# Average the thresholds obtained from all folds\n",
    "average_threshold = np.mean(optimal_thresholds)\n",
    "print(f\"Averaged Optimal Threshold from Cross-Validation: {average_threshold:.4f}\")\n",
    "\n",
    "# Evaluate on the test set using the averaged threshold\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob_test = best_model(data).view(-1)[splits['test']].cpu().detach().numpy()\n",
    "    y_true_test = labels[splits['test']].cpu().detach().numpy()\n",
    "    y_pred_test = (y_prob_test >= average_threshold).astype(int)\n",
    "\n",
    "test_sar_conversion = sar_conversion_rate(y_true_test, y_pred_test)\n",
    "test_recall = recall_score(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Set SAR Conversion Rate at Averaged Threshold: {test_sar_conversion:.4f}\")\n",
    "print(f\"Test Set Recall at Averaged Threshold: {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0e0aa-2a97-466b-921f-0e78ed749b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_recall_tuning = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb89f88-c14f-4d1a-90c7-4495da3d5e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57eb1825-4b55-4cea-bfc6-065d8d8db7f9",
   "metadata": {},
   "source": [
    "# Adding features\n",
    "\n",
    "Alright, without node features, it seems like we cannot do anything. Let's add our tabular data features as node features and let's see if it works better  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096677c-969f-45bb-a959-efcd3e4fc242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17db83d-d48d-4d71-93fa-28674979ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AML_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfff56-09e5-4a6b-909f-0e0496d25efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a32198-0f2f-4935-9f61-8fcffb80aaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c0adc4-63b8-417e-ba35-7df676f1426a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad13137-5304-4dfa-b044-eb70dd5f9538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5e2a4-3a22-4380-9c53-4b383dde08d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.model_selection import ParameterSampler, StratifiedKFold\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "from scipy.stats import randint, uniform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Prepare node features by using the feature columns from `df`\n",
    "node_features = torch.tensor(df.drop(columns=['ACCOUNT_ID', 'IS_FRAUD']).values, dtype=torch.float)\n",
    "\n",
    "# Prepare labels (IS_FRAUD)\n",
    "labels = torch.tensor(df['IS_FRAUD'].astype(int).values, dtype=torch.long)\n",
    "\n",
    "# Prepare edge_index\n",
    "sender_ids = torch.tensor(transac['SENDER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "receiver_ids = torch.tensor(transac['RECEIVER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "edge_index = torch.stack([sender_ids, receiver_ids], dim=0)\n",
    "\n",
    "# Prepare edge features (TX_AMOUNT and TIMESTAMP)\n",
    "edge_attr = torch.tensor(transac[['TX_AMOUNT', 'TIMESTAMP']].values, dtype=torch.float)\n",
    "\n",
    "# Create the PyTorch Geometric data object\n",
    "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the GAT model with edge features\n",
    "class GDPModel(nn.Module):\n",
    "    def __init__(self, num_features=26, hidden_size=32, target_size=1):\n",
    "        super(GDPModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.target_size = target_size\n",
    "        self.convs = nn.ModuleList([\n",
    "            GATConv(self.num_features, self.hidden_size, edge_dim=2),\n",
    "            GATConv(self.hidden_size, self.hidden_size, edge_dim=2)\n",
    "        ])\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr)  # Adding edge features here\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr=edge_attr)  # Adding edge features here\n",
    "        x = self.linear(x)\n",
    "        return torch.sigmoid(x)  # Use sigmoid for binary classification\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = criterion(out.view(-1), data.y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model (focus on recall only)\n",
    "def evaluate(model, data, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        preds = (out.view(-1) > threshold).long()\n",
    "        recall = recall_score(data.y.cpu(), preds.cpu())\n",
    "        return recall\n",
    "\n",
    "# Hyperparameter search space\n",
    "param_dist = {\n",
    "    'hidden_size': randint(16, 128),\n",
    "    'lr': uniform(0.0001, 0.01),\n",
    "    'dropout': uniform(0.1, 0.5),\n",
    "    'epochs': randint(10, 150)\n",
    "}\n",
    "\n",
    "# Randomized search for hyperparameter tuning\n",
    "def randomized_search(data, param_dist, n_iter=10, seed=42):\n",
    "    results = []\n",
    "    sampler = ParameterSampler(param_dist, n_iter=n_iter, random_state=seed)\n",
    "    \n",
    "    for params in sampler:\n",
    "        print(f\"Training with params: {params}\")\n",
    "        model = GDPModel(num_features=data.x.size(1), hidden_size=params['hidden_size']).to(data.x.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        for epoch in range(params['epochs']):\n",
    "            train(model, data, optimizer, criterion)\n",
    "        \n",
    "        recall = evaluate(model, data)\n",
    "        results.append((recall, params))\n",
    "        print(f\"Validation Recall: {recall:.4f}\\n\")\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[0], reverse=True)[0]\n",
    "\n",
    "# Load data and splits (assuming you have already run the split code)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data, splits = random_split_transd_adapted(node_features, labels, edge_index, edge_attr, train_size=0.8, test_size=0.1, device=device)\n",
    "\n",
    "# Perform randomized search for hyperparameter tuning\n",
    "best_recall, best_params = randomized_search(data, param_dist)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation Recall: {best_recall:.4f}\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = GDPModel(num_features=data.x.size(1), hidden_size=best_params['hidden_size']).to(device)\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(best_params['epochs']):\n",
    "    train(best_model, data, optimizer, criterion)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Cross-validation setup with stratification\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "optimal_thresholds = []\n",
    "\n",
    "# Cross-validation process\n",
    "for fold_num, (train_index, val_index) in enumerate(skf.split(data.x.cpu().numpy(), labels.cpu().numpy()), start=1):\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_index] = True\n",
    "    val_mask[val_index] = True\n",
    "    \n",
    "    # Train model on the current fold\n",
    "    best_model.train()\n",
    "    for epoch in range(best_params['epochs']):\n",
    "        train(best_model, data, optimizer, criterion)\n",
    "    \n",
    "    # Predict probabilities on the validation fold\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_prob_val_fold = best_model(data).view(-1)[val_mask].cpu().detach().numpy()\n",
    "        y_true_val_fold = labels[val_mask].cpu().detach().numpy()\n",
    "\n",
    "    # Calculate SAR Conversion Rate and Recall for each threshold\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    sar_rates = []\n",
    "    recalls = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_val = (y_prob_val_fold >= threshold).astype(int)\n",
    "        sar_rate = sar_conversion_rate(y_true_val_fold, y_pred_val)\n",
    "        recall = recall_score(y_true_val_fold, y_pred_val)\n",
    "        sar_rates.append(sar_rate)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Objective recall for the current fold\n",
    "    objective_recall = np.random.uniform(min_recall_tuning, 1.0)\n",
    "    print(f\"Fold {fold_num}: Objective Recall Target = {objective_recall:.4f}\")\n",
    "    \n",
    "    # Find the optimal threshold using the dichotomy method for this fold\n",
    "    optimal_threshold = find_optimal_threshold(y_prob_val_fold, y_true_val_fold, objective_recall)\n",
    "    optimal_thresholds.append(optimal_threshold)\n",
    "\n",
    "    # Plot SAR Conversion Rate and Recall for each threshold\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('SAR Conversion Rate', color='tab:blue')\n",
    "    ax1.plot(thresholds, sar_rates, color='tab:blue', label='SAR Conversion Rate')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('Recall (Class 1)', color='tab:orange')\n",
    "    ax2.plot(thresholds, recalls, color='tab:orange', label='Recall (Class 1)')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Highlight the selected threshold and the objective recall\n",
    "    ax1.axvline(x=optimal_threshold, color='red', linestyle='--', linewidth=1, label='Selected Threshold')\n",
    "    ax2.axhline(y=objective_recall, color='green', linestyle='--', linewidth=1, label='Objective Recall')\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(f'Threshold vs SAR Conversion Rate & Recall (Class 1) - Fold {fold_num}')\n",
    "    plt.show()\n",
    "\n",
    "# Average the thresholds obtained from all folds\n",
    "average_threshold = np.mean(optimal_thresholds)\n",
    "print(f\"Averaged Optimal Threshold from Cross-Validation: {average_threshold:.4f}\")\n",
    "\n",
    "# Evaluate on the test set using the averaged threshold\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob_test = best_model(data).view(-1)[splits['test']].cpu().detach().numpy()\n",
    "    y_true_test = labels[splits['test']].cpu().detach().numpy()\n",
    "    y_pred_test = (y_prob_test >= average_threshold).astype(int)\n",
    "\n",
    "test_sar_conversion = sar_conversion_rate(y_true_test, y_pred_test)\n",
    "test_recall = recall_score(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Set SAR Conversion Rate at Averaged Threshold: {test_sar_conversion:.4f}\")\n",
    "print(f\"Test Set Recall at Averaged Threshold: {test_recall:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28bb00-01db-4bde-b9a0-c68247ce0b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf022dec-10a5-4239-82b4-0ef8fa4420aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7653e2-6fa8-419c-94f4-d0494b002691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7d427-b47a-495d-b8fc-27ddbbc9bd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd640a6-77a6-45f9-a0e8-b02104c36e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b371fb4-de14-40d1-95c7-06a921fb11e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f47ea-8dbd-416a-adee-6ba9970b6fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2f274-3094-426d-be80-f9f3eced5701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82a5a9-712a-4ec2-a4b5-b503906c20a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1cbe7-7644-4909-94a3-377cf640150e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48652f-f8a1-402d-b102-61f9d6f74e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "159125dd-c80e-4d87-9f0b-cf10b5677b0e",
   "metadata": {},
   "source": [
    "## Message passing node classification\n",
    "\n",
    "Sans class imbalance management et entrainement avec une fonction qui pénalise plus la classe 1 que la classe 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d444268-6507-44fc-bb8b-78d039f88649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Prepare edge_index, node features, edge features, and labels as before\n",
    "# Assuming transac and accounts DataFrames are already defined\n",
    "\n",
    "sender_ids = torch.tensor(transac['SENDER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "receiver_ids = torch.tensor(transac['RECEIVER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "edge_index = torch.stack([sender_ids, receiver_ids], dim=0)\n",
    "node_features = torch.tensor(accounts[['INIT_BALANCE']].values, dtype=torch.float)\n",
    "edge_attr = torch.tensor(transac[['TX_AMOUNT', 'TIMESTAMP']].values, dtype=torch.float)\n",
    "labels = torch.tensor(accounts['IS_FRAUD'].astype(int).values, dtype=torch.long)\n",
    "data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=labels)\n",
    "\n",
    "class MPNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super(MPNN, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels + edge_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return F.relu(self.lin(torch.cat([x_j, edge_attr], dim=-1)))\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_dim, num_classes):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mpnn = MPNN(num_node_features, hidden_dim, num_edge_features)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.mpnn(x, edge_index, edge_attr)\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate the model\n",
    "num_node_features = 1  # INIT_BALANCE\n",
    "num_edge_features = 2  # TX_AMOUNT and TIMESTAMP\n",
    "hidden_dim = 32\n",
    "num_classes = 2  # Binary classification\n",
    "\n",
    "model = NodeClassifier(num_node_features, num_edge_features, hidden_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Weighted cross-entropy loss, giving more weight to class 1 (fraud)\n",
    "class_weights = torch.tensor([1.0, 5.0])\n",
    "loss_criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "def sar_conversion_rate(y_true, y_pred):\n",
    "    positive_predictions = np.sum(y_pred)\n",
    "    if positive_predictions == 0:\n",
    "        return 0\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    return true_positives / positive_predictions\n",
    "\n",
    "def find_optimal_threshold(y_prob, y_true, limit_recall, tol=1e-4):\n",
    "    low, high = 0.0, 1.0\n",
    "    best_threshold = 0.5\n",
    "    while high - low > tol:\n",
    "        mid = (low + high) / 2.0\n",
    "        y_pred = (y_prob >= mid).astype(int)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        if recall < limit_recall:\n",
    "            high = mid\n",
    "        else:\n",
    "            best_threshold = mid\n",
    "            low = mid\n",
    "    return best_threshold\n",
    "\n",
    "# Cross-validation setup with stratification\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "optimal_thresholds = []\n",
    "\n",
    "for fold_num, (train_index, val_index) in enumerate(skf.split(data.x.cpu().numpy(), labels.cpu().numpy()), start=1):\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_index] = True\n",
    "    val_mask[val_index] = True\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    model.train()\n",
    "    for epoch in range(50):  # Reduce the number of epochs for faster convergence\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_criterion(out[train_mask], labels[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Predict probabilities on the validation fold\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_prob_val_fold = F.softmax(model(data), dim=1)[:, 1][val_mask].cpu().numpy()\n",
    "        y_true_val_fold = labels[val_mask].cpu().numpy()\n",
    "\n",
    "    # Calculate SAR Conversion Rate and Recall for each threshold\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    sar_rates = []\n",
    "    recalls = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_val = (y_prob_val_fold >= threshold).astype(int)\n",
    "        sar_rate = sar_conversion_rate(y_true_val_fold, y_pred_val)\n",
    "        recall = recall_score(y_true_val_fold, y_pred_val)\n",
    "        sar_rates.append(sar_rate)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Objective recall for the current fold\n",
    "    objective_recall = np.random.uniform(0.8, 1.0)  # Adjust min_recall_tuning as needed\n",
    "    print(f\"Fold {fold_num}: Objective Recall Target = {objective_recall:.4f}\")\n",
    "    \n",
    "    # Find the optimal threshold using the dichotomy method for this fold\n",
    "    optimal_threshold = find_optimal_threshold(y_prob_val_fold, y_true_val_fold, objective_recall)\n",
    "    optimal_thresholds.append(optimal_threshold)\n",
    "\n",
    "    # Plot SAR Conversion Rate and Recall for each threshold\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('SAR Conversion Rate', color='tab:blue')\n",
    "    ax1.plot(thresholds, sar_rates, color='tab:blue', label='SAR Conversion Rate')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('Recall (Class 1)', color='tab:orange')\n",
    "    ax2.plot(thresholds, recalls, color='tab:orange', label='Recall (Class 1)')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Highlight the selected threshold and the objective recall\n",
    "    ax1.axvline(x=optimal_threshold, color='red', linestyle='--', linewidth=1, label='Selected Threshold')\n",
    "    ax2.axhline(y=objective_recall, color='green', linestyle='--', linewidth=1, label='Objective Recall')\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(f'Threshold vs SAR Conversion Rate & Recall (Class 1) - Fold {fold_num}')\n",
    "    plt.show()\n",
    "\n",
    "# Average the thresholds obtained from all folds\n",
    "average_threshold = np.mean(optimal_thresholds)\n",
    "print(f\"Averaged Optimal Threshold from Cross-Validation: {average_threshold:.4f}\")\n",
    "\n",
    "# Evaluate on the test set using the averaged threshold\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob_test = F.softmax(model(data), dim=1)[:, 1][splits['test']].cpu().numpy()\n",
    "    y_true_test = labels[splits['test']].cpu().numpy()\n",
    "    y_pred_test = (y_prob_test >= average_threshold).astype(int)\n",
    "\n",
    "test_sar_conversion = sar_conversion_rate(y_true_test, y_pred_test)\n",
    "test_recall = recall_score(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Set SAR Conversion Rate at Averaged Threshold: {test_sar_conversion:.4f}\")\n",
    "print(f\"Test Set Recall at Averaged Threshold: {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a9296-ceca-42c8-9952-52ac7717e800",
   "metadata": {},
   "source": [
    "<u>Remarque :</u> Le rappel et très instable et le conversion rate reste globalement faible.\n",
    "\n",
    "Il sera donc fondamental d'utiliser des méthdoes de imbalance management et de changer la fonction objectif dans le tuning des hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545573f3-e6a8-49de-99d3-d832caac83e6",
   "metadata": {},
   "source": [
    "# Message passing node classification\n",
    "\n",
    "On utilisera la méthode de sur-échantillonage SMOTE, et l'optisation des hyperpatamètres se fera sur le F1-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4a805-37bb-48a4-8441-9398c9bb6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Prepare edge_index, node features, edge features, and labels\n",
    "sender_ids = torch.tensor(transac['SENDER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "receiver_ids = torch.tensor(transac['RECEIVER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "edge_index = torch.stack([sender_ids, receiver_ids], dim=0)\n",
    "node_features = torch.tensor(accounts[['INIT_BALANCE']].values, dtype=torch.float)\n",
    "edge_attr = torch.tensor(transac[['TX_AMOUNT', 'TIMESTAMP']].values, dtype=torch.float)\n",
    "labels = torch.tensor(accounts['IS_FRAUD'].astype(int).values, dtype=torch.long)\n",
    "\n",
    "# Oversample the data using SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(node_features.cpu().numpy(), labels.cpu().numpy())\n",
    "node_features_resampled = torch.tensor(X_resampled, dtype=torch.float)\n",
    "labels_resampled = torch.tensor(y_resampled, dtype=torch.long)\n",
    "\n",
    "# Create the PyTorch Geometric data object with resampled data\n",
    "data = Data(x=node_features_resampled, edge_index=edge_index, edge_attr=edge_attr, y=labels_resampled)\n",
    "\n",
    "class MPNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super(MPNN, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels + edge_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return F.relu(self.lin(torch.cat([x_j, edge_attr], dim=-1)))\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_dim, num_classes):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mpnn = MPNN(num_node_features, hidden_dim, num_edge_features)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.mpnn(x, edge_index, edge_attr)\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate the model\n",
    "num_node_features = 1  # INIT_BALANCE\n",
    "num_edge_features = 2  # TX_AMOUNT and TIMESTAMP\n",
    "hidden_dim = 32\n",
    "num_classes = 2  # Binary classification\n",
    "\n",
    "model = NodeClassifier(num_node_features, num_edge_features, hidden_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Standard cross-entropy loss\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def sar_conversion_rate(y_true, y_pred):\n",
    "    positive_predictions = np.sum(y_pred)\n",
    "    if positive_predictions == 0:\n",
    "        return 0\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    return true_positives / positive_predictions\n",
    "\n",
    "# Cross-validation setup with stratification\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "optimal_thresholds = []\n",
    "\n",
    "for fold_num, (train_index, val_index) in enumerate(skf.split(data.x.cpu().numpy(), labels_resampled.cpu().numpy()), start=1):\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_index] = True\n",
    "    val_mask[val_index] = True\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    model.train()\n",
    "    for epoch in range(50):  # Reduce the number of epochs for faster convergence\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_criterion(out[train_mask], labels_resampled[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Predict probabilities on the validation fold\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_prob_val_fold = F.softmax(model(data), dim=1)[:, 1][val_mask].cpu().numpy()\n",
    "        y_true_val_fold = labels_resampled[val_mask].cpu().numpy()\n",
    "\n",
    "    # Calculate SAR Conversion Rate and Recall for each threshold\n",
    "    thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "    sar_rates = []\n",
    "    recalls = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred_val = (y_prob_val_fold >= threshold).astype(int)\n",
    "        sar_rate = sar_conversion_rate(y_true_val_fold, y_pred_val)\n",
    "        recall = recall_score(y_true_val_fold, y_pred_val)\n",
    "        sar_rates.append(sar_rate)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Objective recall for the current fold\n",
    "    objective_recall = np.random.uniform(0.8, 1.0)  # Adjust min_recall_tuning as needed\n",
    "    print(f\"Fold {fold_num}: Objective Recall Target = {objective_recall:.4f}\")\n",
    "    \n",
    "    # Find the optimal threshold using the dichotomy method for this fold\n",
    "    optimal_threshold = find_optimal_threshold(y_prob_val_fold, y_true_val_fold, objective_recall)\n",
    "    optimal_thresholds.append(optimal_threshold)\n",
    "\n",
    "    # Plot SAR Conversion Rate and Recall for each threshold\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('SAR Conversion Rate', color='tab:blue')\n",
    "    ax1.plot(thresholds, sar_rates, color='tab:blue', label='SAR Conversion Rate')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.set_ylabel('Recall (Class 1)', color='tab:orange')\n",
    "    ax2.plot(thresholds, recalls, color='tab:orange', label='Recall (Class 1)')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:orange')\n",
    "\n",
    "    # Highlight the selected threshold and the objective recall\n",
    "    ax1.axvline(x=optimal_threshold, color='red', linestyle='--', linewidth=1, label='Selected Threshold')\n",
    "    ax2.axhline(y=objective_recall, color='green', linestyle='--', linewidth=1, label='Objective Recall')\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(f'Threshold vs SAR Conversion Rate & Recall (Class 1) - Fold {fold_num}')\n",
    "    plt.show()\n",
    "\n",
    "# Average the thresholds obtained from all folds\n",
    "average_threshold = np.mean(optimal_thresholds)\n",
    "print(f\"Averaged Optimal Threshold from Cross-Validation: {average_threshold:.4f}\")\n",
    "\n",
    "# Evaluate on the test set using the averaged threshold\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_prob_test = F.softmax(model(data), dim=1)[:, 1][splits['test']].cpu().numpy()\n",
    "    y_true_test = labels_resampled[splits['test']].cpu().numpy()\n",
    "    y_pred_test = (y_prob_test >= average_threshold).astype(int)\n",
    "\n",
    "test_sar_conversion = sar_conversion_rate(y_true_test, y_pred_test)\n",
    "test_recall = recall_score(y_true_test, y_pred_test)\n",
    "test_f1_score = f1_score(y_true_test, y_pred_test)\n",
    "\n",
    "print(f\"Test Set SAR Conversion Rate at Averaged Threshold: {test_sar_conversion:.4f}\")\n",
    "print(f\"Test Set Recall at Averaged Threshold: {test_recall:.4f}\")\n",
    "print(f\"Test Set F1 Score at Averaged Threshold: {test_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5f9adb-75f7-4cf4-9280-1baba735a070",
   "metadata": {},
   "source": [
    "<u>Remarque :</u> La méthode de tuning de threshold que l'on a utilisé en notebook 3 n'est pas adaptée ici. Notre fonction recall est en escalier, avec une marche nous menant à un recall élevée, un palier immense pour un recall un peu faible mais acceptable, puis une marche à recall casi 0. Il nous sera donc difficile de viser un recall extrêmement élevé, la pente étant très inclinée, on a intérêt à choisir le threshold le plus élevé sur la marche de rappel, pour essayer de maximiser le conversion rate.\n",
    "\n",
    "A vu d'oeil, un threshold de 0.7 devrait nous assurer de rester sur ce palier de rappel, voyons donc ce que nous donne un seuil de 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8131fa7-32b9-4b47-84d3-b6752967262e",
   "metadata": {},
   "source": [
    "### MPNN avec SMOTE et optimisation sur f1-score, avec un seuil de décision de 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "616997cd-0aca-49b2-b565-abc21c50ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - SAR Conversion Rate: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Fold 2 - SAR Conversion Rate: 0.9677, Recall: 0.7143, F1 Score: 0.8219\n",
      "Fold 3 - SAR Conversion Rate: 0.9722, Recall: 0.8333, F1 Score: 0.8974\n",
      "Fold 4 - SAR Conversion Rate: 1.0000, Recall: 0.8571, F1 Score: 0.9231\n",
      "Fold 5 - SAR Conversion Rate: 0.5676, Recall: 1.0000, F1 Score: 0.7241\n",
      "Fold 6 - SAR Conversion Rate: 0.9250, Recall: 0.8810, F1 Score: 0.9024\n",
      "Fold 7 - SAR Conversion Rate: 0.5385, Recall: 1.0000, F1 Score: 0.7000\n",
      "Fold 8 - SAR Conversion Rate: 0.6452, Recall: 0.9524, F1 Score: 0.7692\n",
      "Fold 9 - SAR Conversion Rate: 0.9714, Recall: 0.8095, F1 Score: 0.8831\n",
      "Fold 10 - SAR Conversion Rate: 0.8500, Recall: 0.8095, F1 Score: 0.8293\n",
      "Fold 11 - SAR Conversion Rate: 0.9655, Recall: 0.6667, F1 Score: 0.7887\n",
      "Fold 12 - SAR Conversion Rate: 1.0000, Recall: 0.6667, F1 Score: 0.8000\n",
      "Fold 13 - SAR Conversion Rate: 0.9500, Recall: 0.9048, F1 Score: 0.9268\n",
      "Fold 14 - SAR Conversion Rate: 0.6724, Recall: 0.9286, F1 Score: 0.7800\n",
      "Fold 15 - SAR Conversion Rate: 1.0000, Recall: 0.7857, F1 Score: 0.8800\n",
      "Fold 16 - SAR Conversion Rate: 1.0000, Recall: 0.9048, F1 Score: 0.9500\n",
      "Fold 17 - SAR Conversion Rate: 0.9714, Recall: 0.8293, F1 Score: 0.8947\n",
      "Fold 18 - SAR Conversion Rate: 0.9189, Recall: 0.8293, F1 Score: 0.8718\n",
      "Fold 19 - SAR Conversion Rate: 0.5714, Recall: 0.9524, F1 Score: 0.7143\n",
      "Fold 20 - SAR Conversion Rate: 0.9714, Recall: 0.8095, F1 Score: 0.8831\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Prepare edge_index, node features, edge features, and labels\n",
    "sender_ids = torch.tensor(transac['SENDER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "receiver_ids = torch.tensor(transac['RECEIVER_ACCOUNT_ID'].values, dtype=torch.long)\n",
    "edge_index = torch.stack([sender_ids, receiver_ids], dim=0)\n",
    "node_features = torch.tensor(accounts[['INIT_BALANCE']].values, dtype=torch.float)\n",
    "edge_attr = torch.tensor(transac[['TX_AMOUNT', 'TIMESTAMP']].values, dtype=torch.float)\n",
    "labels = torch.tensor(accounts['IS_FRAUD'].astype(int).values, dtype=torch.long)\n",
    "\n",
    "# Oversample the data using SMOTE\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(node_features.cpu().numpy(), labels.cpu().numpy())\n",
    "node_features_resampled = torch.tensor(X_resampled, dtype=torch.float)\n",
    "labels_resampled = torch.tensor(y_resampled, dtype=torch.long)\n",
    "\n",
    "# Create the PyTorch Geometric data object with resampled data\n",
    "data = Data(x=node_features_resampled, edge_index=edge_index, edge_attr=edge_attr, y=labels_resampled)\n",
    "\n",
    "class MPNN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, edge_dim):\n",
    "        super(MPNN, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels + edge_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return F.relu(self.lin(torch.cat([x_j, edge_attr], dim=-1)))\n",
    "\n",
    "class NodeClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_dim, num_classes):\n",
    "        super(NodeClassifier, self).__init__()\n",
    "        self.mpnn = MPNN(num_node_features, hidden_dim, num_edge_features)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = self.mpnn(x, edge_index, edge_attr)\n",
    "        return self.linear(x)\n",
    "\n",
    "# Instantiate the model\n",
    "num_node_features = 1  # INIT_BALANCE\n",
    "num_edge_features = 2  # TX_AMOUNT and TIMESTAMP\n",
    "hidden_dim = 32\n",
    "num_classes = 2  # Binary classification\n",
    "\n",
    "model = NodeClassifier(num_node_features, num_edge_features, hidden_dim, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Standard cross-entropy loss\n",
    "loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def sar_conversion_rate(y_true, y_pred):\n",
    "    positive_predictions = np.sum(y_pred)\n",
    "    if positive_predictions == 0:\n",
    "        return 0\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "    return true_positives / positive_predictions\n",
    "\n",
    "# Cross-validation setup with stratification\n",
    "skf = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_num, (train_index, val_index) in enumerate(skf.split(data.x.cpu().numpy(), labels_resampled.cpu().numpy()), start=1):\n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_index] = True\n",
    "    val_mask[val_index] = True\n",
    "    \n",
    "    # Train the model on the current fold\n",
    "    model.train()\n",
    "    for epoch in range(70): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_criterion(out[train_mask], labels_resampled[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Predict probabilities on the validation fold\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_prob_val_fold = F.softmax(model(data), dim=1)[:, 1][val_mask].cpu().numpy()\n",
    "        y_true_val_fold = labels_resampled[val_mask].cpu().numpy()\n",
    "\n",
    "    # Set the fixed threshold at 0.7\n",
    "    threshold = 0.7\n",
    "    y_pred_val = (y_prob_val_fold >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate SAR Conversion Rate and Recall\n",
    "    sar_rate = sar_conversion_rate(y_true_val_fold, y_pred_val)\n",
    "    recall = recall_score(y_true_val_fold, y_pred_val)\n",
    "    f1 = f1_score(y_true_val_fold, y_pred_val)\n",
    "\n",
    "    print(f\"Fold {fold_num} - SAR Conversion Rate: {sar_rate:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Evaluate on the test set using the fixed threshold\n",
    "#model.eval()\n",
    "#with torch.no_grad():\n",
    "#    y_prob_test = F.softmax(model(data), dim=1)[:, 1][val_mask].cpu().numpy()\n",
    "#    y_true_test = labels_resampled[val_mask].cpu().numpy()\n",
    "#    y_pred_test = (y_prob_test >= threshold).astype(int)\n",
    "\n",
    "#test_sar_conversion = sar_conversion_rate(y_true_test, y_pred_test)\n",
    "#test_recall = recall_score(y_true_test, y_pred_test)\n",
    "#test_f1_score = f1_score(y_true_test, y_pred_test)\n",
    "\n",
    "#print(f\"Test Set SAR Conversion Rate at Threshold 0.7: {test_sar_conversion:.4f}\")\n",
    "#print(f\"Test Set Recall at Threshold 0.7: {test_recall:.4f}\")\n",
    "#print(f\"Test Set F1 Score at Threshold 0.7: {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731392f-401a-4969-97ae-2ba82b732062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06fef94-dcb3-4b9a-8e29-537d39303d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27946a6a-48ee-4661-aed4-b03408bac798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
